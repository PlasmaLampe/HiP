\chapter[Testing the application]{Testing the application}
Although the section called \textit{testing} is the second last of this thesis, testing was a major force within the whole development process. We had to do changes on existing code parts often, which was the main reason for the \ac{TDD} development approach. 

The following section will contain the results of the final test suites for the current version of the HiP application. Thus, this summary does not cope with the importance of testing within the development process but it cannot be shown or expressed in a better way within this written thesis. 

Nevertheless, testing is only a small fragment with respect to a complete quality assurance according to the quality model described within the ISO 9126, which will be described in the next section. However, functional quality assurance (i.e., testing) was most that could be achieved in the time frame of this master thesis. 

\section{Quality assurance and quality models}
Software quality is a term that is hard to grasp and could potentially include very different things. To create a common understanding of quality and to create a formalized expression of quality, \emph{quality models} have been introduced (\cite{waghmodesoftware}). 

The term \emph{quality model} has been defined by Deissenboeck et. al. as \emph{'a model with the objective to describe, assess and/or predict quality'}  (\cite{deissenboeck2009software}). So, as we have said, it is the needed foundation for every common understanding or evaluation of the fuzzy term \emph{quality}.

Although, a multitude of quality models have been proposed and applied within the last years, we will focus on the quality model that has been introduced within the ISO/IEC 9126. This quality model itself is mostly used to define the term \emph{quality}. Furthermore, the ISO/IEC 9126 offers metrics to \emph{measure} the defined quality of the product. However, we will only scratch the surface of this topic within this thesis because software quality models are quite complex.

To define the term quality, the ISO/IEC 9126 defines six characteristics, which are again divided into sub-characteristics. The six main characteristics are \emph{Functionality}, \emph{Reliability}, \emph{Usability}, \emph{Efficience}, \emph{Maintainablity} and \emph{Portability} (\cite{jung2004measuring}). We will not get into details here because we try to keep this section short and the terms should be sufficient to get an intuitive idea about these quality characteristics. The defined sub-characteristics of these main characteristics can be evaluated by the metrics that are proposed within the ISO/IEC 9126. 

To get a better understand of these metrics, we will take a closer look at one sub-characteristic of \emph{Reliability}, which is called \emph{Maturity}. The ISO/IEC 9126 defines a metric for maturity as \emph{'an external maturity metric should be able to measure such attributes as the software freedom of failures caused by faults existing in the software itself'} (\cite{ISOIEC9126}).

As an example, we will take a look at the metric \emph{test coverage} (\cite{ISOIEC9126}), which we will also use in the next section. 

\begin{itemize}
	\item[Name:] Test coverage
	\item[Purpose:] How much of required test cases have been executed during testing?
	\item[Method:] Count the number of test cases performed during testing and compare the number of test cases required to obtain adequate test coverage.
	\item[Measurement:] $X=A/B$, where $A=$Number of actually performed test cases and $B=Number of estimated test cases to be performed to cover all requirements$.
	\item[Interpretation:] $0<=X<=1$: Closer X to $1.0$ is the better test coverage.
	\item[Scale:] Absolute
	\item[Audience:] Developer, Tester, SQA
\end{itemize}

As one can see, the ISO/IEC 9126 defines a metric with a name, shows the purpose and explains how the metric can be used and evaluated by the target audience. By using these metrics, we can evaluate the quality of the sub-characteristic. The important thing is that the main-characteristics are created from these sub-characteristics. So, we evaluate the quality of the main characteristics. Thus, we evaluate by definition the quality of our whole software product by using this metrics because we defined software quality as the quality of the main characteristics.        

So, after we have now seen this theoretical approach to software quality assurance, we will now take a closer look at the used test suites. As we have described in chapter \ref{background}, the tests have been developed with the help of the Jasmine framework.
  
\section{Test environment}
The Jasmine test suites were run within Karma on Mac OS X operating system with Google Chrome. The hardware configuration was a dual core \ac{CPU} and 8096MB \ac{RAM}.

\section{Testing results}
A typical test case within our test suites looks like the test case shown in Listing \ref{testcase}. 

\lstset{language=Java,
basicstyle=\small,
showspaces=false,
showstringspaces=false,   
tabsize=2,
backgroundcolor=\color{grey}}
\begin{lstlisting}[numbers=left,caption={Simple test case for the type service},label=testcase,frame=tlbr,breaklines]
    it('is able to fetch all types', function () {
        var check = function(type){
            expect(type.length).toBe(2);
        };
        service.getTypes(check);

        $httpBackend.expect("GET","/admin/types").respond(200, typeList);
        $httpBackend.flush();
    });
\end{lstlisting}

As one can see, this is a unit test case written for the Jasmine framework. Line 1 shows the header of the test case, which can be read like a typical english sentence. The body of the test case contains the call of the actual function (line 5) and the matching against the expected value (line 3). Furthermore, one can easily check the commands that have been send via the \ac{REST} interface with the mockup of the \ac{HTTP} module of AngularJS, which is called httpBackend in the listing.

Because we restricted ourselves to unit tests and did not create any integration tests, we were not able to reach every line of code within our controllers and services within the unit test cases (a lot of code is only for handling events and user input). This can also be seen within the implementation of the acceptance criteria, which has been created from the requirements engineering (like the requirements in chapter \ref{draft}, these criteria are shown in tables \ref{RequirementsBackendSupervisor}, \ref{RequirementsBackendStudent}, \ref{RequirementsBackendMaster}, \ref{RequirementsBackendMisc} and \ref{RequirementsFrontend}). Some of these acceptance criteria could only be automatically tested with integration tests and are, thus, not tested within this thesis. 

However, we ended up with XX\todo{set} test cases with a code-coverage of YY\todo{set}. Remember, the defined metric \emph{test coverage} uses the ratio between the actual amount of test cases and the estimated amount of test cases needed to cover all requirements. We estimate this number to be about ZZ\todo{set}. So our ratio ends up to be \todo{set}. Because we should get as close as $1.0$ as possible, the value is quite fine.

Unit testing was not the only part of quality assurance that we have used for this project. The next section will describe the usability improvements of the \ac{HiP} application.

\section{Usability study of the backend system}
According to Lin et. al., usability is an important part of every software system and the importance of usability for such systems is still rising (\cite{lin1997proposed}). Furthermore, \emph{Usability} is a main characteristic within the quality model of the ISO/IEC 9126.

The ISO/IEC 9126 defines the term usability as 'a set of attributes that bear on the effort needed for use, and on the individual assessment of such use, by a stated or implied set of users' (\cite{bevan1997quality}). The sub-characteristics of \emph{Usability}) are defined as \emph{Understandability}, \emph{Learnability}, \emph{Operability}, \emph{Attractiveness} and \emph{Usability Compliance} (\cite{bevan1997quality}). In general, it is very hard to evaluate usability problems by developers for systems they have written because they have a very different view as the end-user. This problem gets enhanced by the fact that usability problems are in many cases hard to grasp by objective metrics and are influenced by personal and cultural backgrounds (\cite{herman1996towards}). The most objective way to evaluate a lot of usability problems are checklists and questionnaires (e.g., did we include tooltips to increase the \emph{Learnability} of the application?) (\cite{herman1996towards})\todo{better source?}. 

Because of these problems, we tried to get professional feedback by an external usability expert to circumvent the problem that we need to evaluate our own system. Kindly, M. Sc. Bj\"orn Senft offered us an evaluation of the \ac{HiP} application backend within an informal review session to find common usability problems. 

Together with one member of the currently started project group, which will take over the development project, we sat together and made a complete 3 hour walkthrough of the application. In the end, we came up with a great list of small and not that small usability problems. Examples of such problems were, the missing of instant feedback for a couple of user forms and misleading color conventions. Most of these problems have been fixed in the last weeks of this master-thesis. However, some of these problems remain open and are listet in Table \ref{usabilityTable} within the appendix. As we will see in section \ref{futurework}, these problems may be handled by the project group.

